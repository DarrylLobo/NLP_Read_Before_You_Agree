{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.22.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Embross\n",
      "[nltk_data]     Employee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from autocorrect import Speller\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pycaret\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "#loading classification model, 17 categories\n",
    "with open('./model-2.pickle','rb') as f:\n",
    "    model2 = pickle.load(f)\n",
    "\n",
    "#loading punitive model, 1 or 0\n",
    "with open('./model-1-v1.pickle','rb') as f:\n",
    "    model1 = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions used in this script\n",
    "\n",
    "# created but can assume these companies spelled it right in there TOA\n",
    "def spelling(text):\n",
    "    \n",
    "    spell = Speller(lang='en')\n",
    "    return str(spell(text))\n",
    "    \n",
    "def tokening(text):\n",
    "    \n",
    "    #nltk.download('punkt')\n",
    "    tokenized_word=word_tokenize(text)\n",
    "    return tokenized_word\n",
    "    \n",
    "def removing_stopwords(text):\n",
    "    \n",
    "    #nltk.download('punkt')\n",
    "    tokenized_word1=tokening(text)\n",
    "    #nltk.download('stopwords')\n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    filtered_sent=[]\n",
    "    for w in tokenized_word1:\n",
    "        if w not in stop_words:\n",
    "            filtered_sent.append(w)\n",
    "    return \" \".join(filtered_sent)\n",
    "\n",
    "\n",
    "\n",
    "def Lemmating(text):\n",
    "    #nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_word1=tokening(text)\n",
    "    stemmed_words=[]\n",
    "    for w in tokenized_word1:\n",
    "        stemmed_words.append(lemmatizer.lemmatize(w))\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "def pos_tag(text):\n",
    "    #nltk.download('averaged_perceptron_tagger')\n",
    "    tokenized_word1=tokening(text)\n",
    "    pos=nltk.pos_tag(removing_stopwords(text1))\n",
    "    return pos\n",
    "    \n",
    "# function called for cleaning html tags from text\n",
    "def cleanhtml(raw_html):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  cleantext = re.sub(cleanr, '', raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingSentence(sentence):\n",
    "    #remove all tags like <p> etc. in the data frame \"data_sentenced\"\n",
    "    sentence = cleanhtml(sentence)\n",
    "    # remove any non alpha numeric\n",
    "    sentence = re.sub('[^0-9a-zA-Z]+', ' ', sentence)\n",
    "    #remove stop words\n",
    "    sentence = removing_stopwords(sentence)\n",
    "    #Lemmaing words\n",
    "    sentence = Lemmating(sentence)\n",
    "    return sentence  \n",
    "    \n",
    "#punitive model, to see whether the sentence is punitive or not\n",
    "def punitivePredict(sentences):\n",
    "    #input list of sentence, get predict value back from model\n",
    "    prediction = model1.predict(sentences)[0]\n",
    "    #return list of result matches with input order\n",
    "    return prediction\n",
    "\n",
    "#classification model based on 17 categories\n",
    "def classifyPredict(sentences):\n",
    "    #input list of sentence, get predict value back from model\n",
    "    prediction = model2.predict(sentences)[0]\n",
    "    #return list of result matches with input order\n",
    "    return prediction\n",
    "\n",
    "#creating company vectors, number of rows = len(sentences), number of columns = number of categories\n",
    "#if the sentence is punitive, then modifies the value\n",
    "#if the sentence is not punitive, then do not change that row\n",
    "def createCompanyVectors(punitivePredictions, classifyPredictions):\n",
    "    companyVector = np.full((len(punitivePredictions), 17), 0)\n",
    "    for i in range(len(punitivePredictions)):\n",
    "        if punitivePredictions[i] == 1:\n",
    "            companyVector[i][classifyPredictions[i]] = 1\n",
    "\n",
    "    return companyVector\n",
    "\n",
    "#calculating the cosine similarity\n",
    "def cos_sim(a, b):\n",
    "    \"\"\"Takes 2 vectors a, b and returns the cosine similarity \n",
    "    \"\"\"\n",
    "    dot_product = np.dot(a, b) # x.y\n",
    "    norm_a = np.linalg.norm(a) #|x|\n",
    "    norm_b = np.linalg.norm(b) #|y|\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "with open('WeChatTC.txt', 'r') as f:\n",
    "    paragraph = f.read()\n",
    "\n",
    "#split to sentence from paragrah\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "preprocessedSentences = []\n",
    "result = []\n",
    "#result.append('sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the sentence by defined functions\n",
    "for i in range(len(sentences)):\n",
    "    preprocessedSentence = preprocessingSentence(sentences[i])\n",
    "    preprocessedSentences.append(preprocessedSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c96db1477b4ba7a09f4f1be98e13a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea1a14e72654afbb323aa912f41aade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c068cfba3ee4d39b83026f763806c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e73fc673154d219201f76951e27bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#get punitive result\n",
    "punitivePredictions = punitivePredict(preprocessedSentences)\n",
    "#get classification result\n",
    "classifyPredictions = classifyPredict(preprocessedSentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get company vector based on punitive result and classification result\n",
    "company_vector = createCompanyVectors(punitivePredictions, classifyPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get company ethics vector from company vector\n",
    "company_ethics_vector = np.ones((len(punitivePredictions), 17))- company_vector\n",
    "company_ethics_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalized company ethics vector - get avergae value\n",
    "company_ethics_vector = company_ethics_vector.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User comes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get user vector based on 17 categories\n",
    "user_vector = np.array([0, 0, 0,5,0,10,0,0,0,0,0,0,0,0,0,0,0])\n",
    "# normalize by diving array by 10 which is the highest number\n",
    "user_vector = user_vector/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity : 30.26%\n"
     ]
    }
   ],
   "source": [
    "#how close between user preference and company vector - cosine similarity\n",
    "print('Cosine Similarity : {:.2f}%'.format(cos_sim(company_ethics_vector, user_vector)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
